# AISHOW - Unity Showrunner

## Overview
Build AI-powered virtual production app in Unity powered by dynamic scripts generated by AI from JSON coming from web endpoints.  

![image](https://github.com/user-attachments/assets/7adcc2b0-9957-4467-811b-0861fff04158)


![image](https://hackmd.io/_uploads/By0Ounc71x.png)

`Unity 2022.3.53f1`

## Project Overview

## References

- **Video Inspiration**: [YouTube Video](https://www.youtube.com/watch?v=zD9wofGof80)
- **Software Dependencies**:
  - Unity - `Unity 2022.3.53f1` https://unity.com/download
  - uniVRM VRM 1.0 - https://github.com/vrm-c/UniVRM/releases/tag/v0.128.0
  - Sithlords Showrunner Framework for generating JSON https://hackmd.io/@smsithlord/Hk7NOUrmke

## Unity Objectives
- [X] Create Unity Streaming App Base Project (Unity 2022.3.53f1) 
- [X] Import uniVRM, test VRMs
- [X] Define 3â€“4 character personalities and create placeholder models for testing core interactions and scripts.
- [X] Generate scripts and episodes in advance to ensure smoother pipelines. Live episodes can remain a Phase 2 or 3 stretch goal.
- [X] Setup Basic Idle Animations
- [X] Setup Camera Switching, and `LookAtLogic`
- [X] Setup PayloadManger (Parsing JSON from web endpoint into useful functions)
- [X] Start with simple text-based interactions.
- [X] Create Event Listeners and Calls to process the payload
- [X] Test JSON-based script generation and camera cuts in Unity.
- [X] Create StateManagers for Cameras, and Dialog
- [X] Create UI to see text parsed from payload and debugging
- [X] Collect 3d World Assets, Props
- [X] Collect VRM Avatars
- [X] Develop an MVP / Demo for the 24/7 livestream.
- [ ] Create documentation for all of the code and API usage examples
- [X] Define personalities for each character.
- [X] Generate scripts via SMSithlords tooling
- [X] Decide on themes or key areas of interaction for characters.
- [X] Each character needs a unique backstory and voice for meaningful interaction with AI scripts. Embedding this locally seems optimal.
- [X] Scripts and episodes can be pre-generated for smoother pipelines.
- [X] Added Media Texture Loading thru "speak" event

      
- Live episodes remain a stretch goal.

## Screenshots

![Unity_yNCachdOqO](https://github.com/user-attachments/assets/064db9f9-fe6f-4fb7-b50f-4cc1f69fb8ef)


### Technical Goals

- Unity for scene control due to VRM animation support.
- Utilize OBS for recording videos
- Use (Elevenlabs) for TTS
- VRM animation in Unity to achieve quality lipsync and character expressions.

## Framework 
- The 3D visualization framework uses **Unity** for rendering.
- Sithlords AI showrunner framework runs on **client-side JavaScript** in a web browser. https://hackmd.io/@smsithlord/Hk7NOUrmke
  - It sends **async calls** to handle:
    - **Scene loading:** Includes location, list of actors, and named spawn points.
      - Actors can spawn randomly if no spawn location is specified.
      - The stage responds with a `loadSceneComplete` event once finished.
    - **Dialogue lines:** Specifies the actor speaking and their line.
      - A TTS system speaks the line and fires a `speakComplete` event after finishing.

### TTS (Text-to-Speech) Integration
- **ElevenLabs**

---

#### Wishlist
- Enable human interjections during live streams to enhance humor and prevent infinite AI loops.
- Hook Eliza agents to the system for interactive conversational capabilities.
- Develop a "Director Mode" where humans can influence live scenes.
- Dynamic Voting System - Replace chatbot-like real-time interactivity with a voting system for scene choices. This scales better and simplifies viewer engagement.
- Build an `AgentState();` function:
- Monitor and manage the state of AI agents.
- Allow toggling between AI-driven and human-driven interactions.

